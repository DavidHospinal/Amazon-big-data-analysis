{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f6d32e9-bc1c-4e7e-bdea-3b90465929e3",
   "metadata": {},
   "source": [
    "# ğŸ“‹ Notebook 2: Preprocesamiento de Datos - Amazon Reviews Analysis\n",
    "\n",
    "**Curso:** INF3590 - Big Data  \n",
    "**Universidad:** Pontificia Universidad CatÃ³lica de Chile  \n",
    "**Estudiante:** Oscar David Hospinal R.  \n",
    "**Fecha:** Junio 2025\n",
    "\n",
    "## ğŸ¯ Objetivo\n",
    "Limpiar, transformar y enriquecer los datos de reviews de Amazon obtenidos en el notebook anterior, preparÃ¡ndolos para el almacenamiento en base NoSQL.\n",
    "\n",
    "## ğŸ“Š Datos de Entrada\n",
    "- **Fuente:** Archivos JSON procesados desde Notebook 1\n",
    "- **Registros:** 1,200 reviews (200 por categorÃ­a)\n",
    "- **CategorÃ­as:** 6 (Books, Video Games, Movies & TV, Home & Kitchen, Tools & Home Improvement, Patio Lawn & Garden)\n",
    "\n",
    "## ğŸ”§ TÃ©cnicas de Preprocesamiento\n",
    "1. **Limpieza:** EliminaciÃ³n de registros incompletos/duplicados\n",
    "2. **TransformaciÃ³n:** NormalizaciÃ³n de formatos y tipos de datos\n",
    "3. **Enriquecimiento:** CreaciÃ³n de nuevas variables categÃ³ricas\n",
    "4. **ValidaciÃ³n:** VerificaciÃ³n de calidad e integridad de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac45ca66-62c9-46dc-9c30-aefec4e9190a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ConfiguraciÃ³n inicial completada\n",
      "ğŸ“ Directorio base: D:\\Proyectos con IA\\May25\\Proy03-Tarea 01 Amazon-Big-Data\\amazon-big-data\n",
      "ğŸ“Š CategorÃ­as a procesar: 6\n"
     ]
    }
   ],
   "source": [
    "# ===== CELDA 1: CONFIGURACIÃ“N INICIAL Y IMPORTS =====\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ConfiguraciÃ³n de paths\n",
    "BASE_PATH = Path(os.getcwd()).parent\n",
    "DATA_PATH = BASE_PATH / \"data\"\n",
    "PROCESSED_PATH = DATA_PATH / \"processed\"\n",
    "SAMPLES_PATH = DATA_PATH / \"samples\"\n",
    "\n",
    "# Crear directorios si no existen\n",
    "SAMPLES_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# ConfiguraciÃ³n de categorÃ­as\n",
    "CATEGORIES = {\n",
    "    'Books': 'Entertainment',\n",
    "    'Video_Games': 'Entertainment', \n",
    "    'Movies_and_TV': 'Entertainment',\n",
    "    'Home_and_Kitchen': 'Home',\n",
    "    'Tools_and_Home_Improvement': 'Home',\n",
    "    'Patio_Lawn_and_Garden': 'Home'\n",
    "}\n",
    "\n",
    "print(\"âœ… ConfiguraciÃ³n inicial completada\")\n",
    "print(f\"ğŸ“ Directorio base: {BASE_PATH}\")\n",
    "print(f\"ğŸ“Š CategorÃ­as a procesar: {len(CATEGORIES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdf0b9ad-0c77-4e3f-9541-75bb0b1ef979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Funciones de limpieza y transformaciÃ³n definidas\n"
     ]
    }
   ],
   "source": [
    "# ===== CELDA 2: FUNCIONES DE LIMPIEZA Y TRANSFORMACIÃ“N =====\n",
    "\n",
    "def clean_review_data(review_data):\n",
    "    \"\"\"\n",
    "    Limpia y valida un registro de review individual\n",
    "    \"\"\"\n",
    "    cleaned_review = {}\n",
    "    \n",
    "    # Campos obligatorios con valores por defecto\n",
    "    cleaned_review['reviewerID'] = review_data.get('reviewerID', 'UNKNOWN')\n",
    "    cleaned_review['asin'] = review_data.get('asin', 'UNKNOWN')\n",
    "    cleaned_review['reviewerName'] = review_data.get('reviewerName', 'Anonymous')\n",
    "    \n",
    "    # Limpieza de campo helpful\n",
    "    helpful = review_data.get('helpful', [0, 0])\n",
    "    if isinstance(helpful, list) and len(helpful) >= 2:\n",
    "        cleaned_review['helpful'] = helpful[:2]  # Solo primeros 2 elementos\n",
    "    else:\n",
    "        cleaned_review['helpful'] = [0, 0]\n",
    "    \n",
    "    # Limpieza de texto de review\n",
    "    review_text = review_data.get('reviewText', '')\n",
    "    if isinstance(review_text, str):\n",
    "        cleaned_review['reviewText'] = review_text.strip()[:1000]  # Limitar a 1000 chars\n",
    "    else:\n",
    "        cleaned_review['reviewText'] = ''\n",
    "    \n",
    "    # ValidaciÃ³n y normalizaciÃ³n de rating\n",
    "    overall = review_data.get('overall', 3.0)\n",
    "    try:\n",
    "        overall_float = float(overall)\n",
    "        if 1.0 <= overall_float <= 5.0:\n",
    "            cleaned_review['overall'] = overall_float\n",
    "        else:\n",
    "            cleaned_review['overall'] = 3.0  # Valor por defecto\n",
    "    except (ValueError, TypeError):\n",
    "        cleaned_review['overall'] = 3.0\n",
    "    \n",
    "    # Limpieza de summary\n",
    "    summary = review_data.get('summary', '')\n",
    "    if isinstance(summary, str):\n",
    "        cleaned_review['summary'] = summary.strip()[:200]  # Limitar a 200 chars\n",
    "    else:\n",
    "        cleaned_review['summary'] = ''\n",
    "    \n",
    "    # ValidaciÃ³n de timestamp\n",
    "    unix_time = review_data.get('unixReviewTime', 0)\n",
    "    try:\n",
    "        cleaned_review['unixReviewTime'] = int(unix_time)\n",
    "    except (ValueError, TypeError):\n",
    "        cleaned_review['unixReviewTime'] = 0\n",
    "    \n",
    "    # Limpieza de fecha legible\n",
    "    review_time = review_data.get('reviewTime', '')\n",
    "    if isinstance(review_time, str):\n",
    "        cleaned_review['reviewTime'] = review_time.strip()\n",
    "    else:\n",
    "        cleaned_review['reviewTime'] = ''\n",
    "    \n",
    "    return cleaned_review\n",
    "\n",
    "def enrich_review_data(review_data, category_name):\n",
    "    \"\"\"\n",
    "    Enriquece el registro con campos adicionales\n",
    "    \"\"\"\n",
    "    # Agregar grupo de categorÃ­a\n",
    "    review_data['category_group'] = CATEGORIES.get(category_name, 'Other')\n",
    "    \n",
    "    # Agregar tipo de anÃ¡lisis basado en grupo\n",
    "    if review_data['category_group'] == 'Entertainment':\n",
    "        review_data['analysis_type'] = 'Leisure/Personal'\n",
    "    elif review_data['category_group'] == 'Home':\n",
    "        review_data['analysis_type'] = 'Practical/Utility'\n",
    "    else:\n",
    "        review_data['analysis_type'] = 'General'\n",
    "    \n",
    "    # Agregar timestamp de procesamiento\n",
    "    review_data['download_timestamp'] = datetime.now().timestamp()\n",
    "    \n",
    "    # Agregar categorÃ­a original\n",
    "    review_data['original_category'] = category_name\n",
    "    \n",
    "    return review_data\n",
    "\n",
    "def validate_review_quality(review_data):\n",
    "    \"\"\"\n",
    "    Valida la calidad del registro de review\n",
    "    Retorna True si el registro es vÃ¡lido\n",
    "    \"\"\"\n",
    "    # Verificar campos obligatorios\n",
    "    required_fields = ['reviewerID', 'asin', 'overall']\n",
    "    for field in required_fields:\n",
    "        if field not in review_data or not review_data[field]:\n",
    "            return False\n",
    "    \n",
    "    # Verificar que overall estÃ© en rango vÃ¡lido\n",
    "    if not (1.0 <= review_data['overall'] <= 5.0):\n",
    "        return False\n",
    "    \n",
    "    # Verificar que tenga contenido mÃ­nimo (texto o summary)\n",
    "    has_content = (review_data.get('reviewText', '').strip() or \n",
    "                   review_data.get('summary', '').strip())\n",
    "    \n",
    "    return bool(has_content)\n",
    "\n",
    "print(\"âœ… Funciones de limpieza y transformaciÃ³n definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26c3faa6-72cb-4788-9090-95ee97be3231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Iniciando procesamiento por categorÃ­as...\n",
      "\n",
      "ğŸ“ Procesando: Books (Entertainment)\n",
      "   ğŸ“Š Cargados: 200 | VÃ¡lidos: 200 | InvÃ¡lidos: 0\n",
      "ğŸ“ Procesando: Video_Games (Entertainment)\n",
      "   ğŸ“Š Cargados: 200 | VÃ¡lidos: 200 | InvÃ¡lidos: 0\n",
      "ğŸ“ Procesando: Movies_and_TV (Entertainment)\n",
      "   ğŸ“Š Cargados: 200 | VÃ¡lidos: 200 | InvÃ¡lidos: 0\n",
      "ğŸ“ Procesando: Home_and_Kitchen (Home)\n",
      "   ğŸ“Š Cargados: 200 | VÃ¡lidos: 200 | InvÃ¡lidos: 0\n",
      "ğŸ“ Procesando: Tools_and_Home_Improvement (Home)\n",
      "   ğŸ“Š Cargados: 200 | VÃ¡lidos: 200 | InvÃ¡lidos: 0\n",
      "ğŸ“ Procesando: Patio_Lawn_and_Garden (Home)\n",
      "   ğŸ“Š Cargados: 200 | VÃ¡lidos: 200 | InvÃ¡lidos: 0\n",
      "\n",
      "âœ… Procesamiento completado:\n",
      "ğŸ“ˆ Total registros cargados: 1200\n",
      "ğŸ§¹ Total registros limpiados: 1200\n",
      "âœ… Total registros vÃ¡lidos: 1200\n",
      "âŒ Total registros descartados: 0\n"
     ]
    }
   ],
   "source": [
    "# ===== CELDA 3: CARGA Y PROCESAMIENTO DE DATOS POR CATEGORÃA =====\n",
    "\n",
    "all_processed_reviews = []\n",
    "processing_stats = {\n",
    "    'total_loaded': 0,\n",
    "    'total_cleaned': 0,\n",
    "    'total_valid': 0,\n",
    "    'by_category': {}\n",
    "}\n",
    "\n",
    "print(\"ğŸ”„ Iniciando procesamiento por categorÃ­as...\\n\")\n",
    "\n",
    "for category_name, group_name in CATEGORIES.items():\n",
    "    file_path = PROCESSED_PATH / f\"{category_name}_sample.json\"\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"âš ï¸  Archivo no encontrado: {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"ğŸ“ Procesando: {category_name} ({group_name})\")\n",
    "    \n",
    "    # Cargar datos de la categorÃ­a\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        category_reviews = json.load(f)\n",
    "    \n",
    "    category_stats = {\n",
    "        'loaded': len(category_reviews),\n",
    "        'cleaned': 0,\n",
    "        'valid': 0,\n",
    "        'invalid': 0\n",
    "    }\n",
    "    \n",
    "    # Procesar cada review de la categorÃ­a\n",
    "    for review in category_reviews:\n",
    "        # Paso 1: Limpiar datos\n",
    "        cleaned_review = clean_review_data(review)\n",
    "        category_stats['cleaned'] += 1\n",
    "        \n",
    "        # Paso 2: Enriquecer con campos adicionales\n",
    "        enriched_review = enrich_review_data(cleaned_review, category_name)\n",
    "        \n",
    "        # Paso 3: Validar calidad\n",
    "        if validate_review_quality(enriched_review):\n",
    "            all_processed_reviews.append(enriched_review)\n",
    "            category_stats['valid'] += 1\n",
    "        else:\n",
    "            category_stats['invalid'] += 1\n",
    "    \n",
    "    # Actualizar estadÃ­sticas globales\n",
    "    processing_stats['total_loaded'] += category_stats['loaded']\n",
    "    processing_stats['total_cleaned'] += category_stats['cleaned']\n",
    "    processing_stats['total_valid'] += category_stats['valid']\n",
    "    processing_stats['by_category'][category_name] = category_stats\n",
    "    \n",
    "    print(f\"   ğŸ“Š Cargados: {category_stats['loaded']} | VÃ¡lidos: {category_stats['valid']} | InvÃ¡lidos: {category_stats['invalid']}\")\n",
    "\n",
    "print(f\"\\nâœ… Procesamiento completado:\")\n",
    "print(f\"ğŸ“ˆ Total registros cargados: {processing_stats['total_loaded']}\")\n",
    "print(f\"ğŸ§¹ Total registros limpiados: {processing_stats['total_cleaned']}\")\n",
    "print(f\"âœ… Total registros vÃ¡lidos: {processing_stats['total_valid']}\")\n",
    "print(f\"âŒ Total registros descartados: {processing_stats['total_loaded'] - processing_stats['total_valid']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2be0421-7cd0-4c85-8ac9-4664d043d63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ANÃLISIS DE CALIDAD DE DATOS PROCESADOS\n",
      "\n",
      "ğŸ” ESTADÃSTICAS GENERALES:\n",
      "   Total de registros: 1,200\n",
      "   CategorÃ­as Ãºnicas: 6\n",
      "   Grupos de anÃ¡lisis: 2\n",
      "\n",
      "ğŸ“ˆ DISTRIBUCIÃ“N POR CATEGORÃA:\n",
      "   Books                     | Entertainment | 200 registros\n",
      "   Video_Games               | Entertainment | 200 registros\n",
      "   Movies_and_TV             | Entertainment | 200 registros\n",
      "   Home_and_Kitchen          | Home         | 200 registros\n",
      "   Tools_and_Home_Improvement | Home         | 200 registros\n",
      "   Patio_Lawn_and_Garden     | Home         | 200 registros\n",
      "\n",
      "ğŸ“Š DISTRIBUCIÃ“N POR GRUPO:\n",
      "   Entertainment | 600 registros |  50.0%\n",
      "   Home         | 600 registros |  50.0%\n",
      "\n",
      "â­ DISTRIBUCIÃ“N DE RATINGS:\n",
      "   Promedio: 4.39â­\n",
      "   Mediana:  5.00â­\n",
      "   MÃ­nimo:   1.0â­\n",
      "   MÃ¡ximo:   5.0â­\n",
      "\n",
      "ğŸ“ CALIDAD DE CONTENIDO:\n",
      "   Reviews con texto: 1,198 (99.8%)\n",
      "   Reviews con summary: 1,200 (100.0%)\n",
      "   Longitud promedio texto: 421 caracteres\n",
      "   Longitud promedio summary: 24 caracteres\n",
      "\n",
      "ğŸ‘¤ INFORMACIÃ“N DE REVIEWERS:\n",
      "   Reviewers Ãºnicos: 1,158\n",
      "   Reviewers anÃ³nimos: 5 (0.4%)\n"
     ]
    }
   ],
   "source": [
    "# ===== CELDA 4: ANÃLISIS DE CALIDAD DE DATOS =====\n",
    "\n",
    "print(\"ğŸ“Š ANÃLISIS DE CALIDAD DE DATOS PROCESADOS\\n\")\n",
    "\n",
    "# Convertir a DataFrame para anÃ¡lisis\n",
    "df_processed = pd.DataFrame(all_processed_reviews)\n",
    "\n",
    "print(\"ğŸ” ESTADÃSTICAS GENERALES:\")\n",
    "print(f\"   Total de registros: {len(df_processed):,}\")\n",
    "print(f\"   CategorÃ­as Ãºnicas: {df_processed['original_category'].nunique()}\")\n",
    "print(f\"   Grupos de anÃ¡lisis: {df_processed['category_group'].nunique()}\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ DISTRIBUCIÃ“N POR CATEGORÃA:\")\n",
    "category_dist = df_processed['original_category'].value_counts()\n",
    "for category, count in category_dist.items():\n",
    "    group = CATEGORIES[category]\n",
    "    print(f\"   {category:25} | {group:12} | {count:3} registros\")\n",
    "\n",
    "print(\"\\nğŸ“Š DISTRIBUCIÃ“N POR GRUPO:\")\n",
    "group_dist = df_processed['category_group'].value_counts()\n",
    "for group, count in group_dist.items():\n",
    "    percentage = (count / len(df_processed)) * 100\n",
    "    print(f\"   {group:12} | {count:3} registros | {percentage:5.1f}%\")\n",
    "\n",
    "print(\"\\nâ­ DISTRIBUCIÃ“N DE RATINGS:\")\n",
    "rating_stats = df_processed['overall'].describe()\n",
    "print(f\"   Promedio: {rating_stats['mean']:.2f}â­\")\n",
    "print(f\"   Mediana:  {rating_stats['50%']:.2f}â­\")\n",
    "print(f\"   MÃ­nimo:   {rating_stats['min']:.1f}â­\")\n",
    "print(f\"   MÃ¡ximo:   {rating_stats['max']:.1f}â­\")\n",
    "\n",
    "print(\"\\nğŸ“ CALIDAD DE CONTENIDO:\")\n",
    "has_review_text = df_processed['reviewText'].str.len() > 0\n",
    "has_summary = df_processed['summary'].str.len() > 0\n",
    "avg_review_length = df_processed[has_review_text]['reviewText'].str.len().mean()\n",
    "avg_summary_length = df_processed[has_summary]['summary'].str.len().mean()\n",
    "\n",
    "print(f\"   Reviews con texto: {has_review_text.sum():,} ({(has_review_text.mean()*100):.1f}%)\")\n",
    "print(f\"   Reviews con summary: {has_summary.sum():,} ({(has_summary.mean()*100):.1f}%)\")\n",
    "print(f\"   Longitud promedio texto: {avg_review_length:.0f} caracteres\")\n",
    "print(f\"   Longitud promedio summary: {avg_summary_length:.0f} caracteres\")\n",
    "\n",
    "print(\"\\nğŸ‘¤ INFORMACIÃ“N DE REVIEWERS:\")\n",
    "unique_reviewers = df_processed['reviewerID'].nunique()\n",
    "anonymous_reviewers = (df_processed['reviewerName'] == 'Anonymous').sum()\n",
    "print(f\"   Reviewers Ãºnicos: {unique_reviewers:,}\")\n",
    "print(f\"   Reviewers anÃ³nimos: {anonymous_reviewers:,} ({(anonymous_reviewers/len(df_processed)*100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d10b82f-fe8d-4878-af36-d49c1a319679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” VALIDACIÃ“N DE CAMPOS ENRIQUECIDOS\n",
      "\n",
      "âœ… VERIFICACIÃ“N DE CAMPOS NUEVOS:\n",
      "   category_group       | 1,200/1,200 registros completos\n",
      "   analysis_type        | 1,200/1,200 registros completos\n",
      "   download_timestamp   | 1,200/1,200 registros completos\n",
      "   original_category    | 1,200/1,200 registros completos\n",
      "\n",
      "ğŸ“Š VALIDACIÃ“N DE CATEGORIZACIÃ“N:\n",
      "   Books                     â†’ Entertainment | 200 registros âœ…\n",
      "   Home_and_Kitchen          â†’ Home         | 200 registros âœ…\n",
      "   Movies_and_TV             â†’ Entertainment | 200 registros âœ…\n",
      "   Patio_Lawn_and_Garden     â†’ Home         | 200 registros âœ…\n",
      "   Tools_and_Home_Improvement â†’ Home         | 200 registros âœ…\n",
      "   Video_Games               â†’ Entertainment | 200 registros âœ…\n",
      "\n",
      "ğŸ¯ VALIDACIÃ“N DE TIPO DE ANÃLISIS:\n",
      "   Entertainment â†’ Leisure/Personal | 600 registros âœ…\n",
      "   Home         â†’ Practical/Utility | 600 registros âœ…\n",
      "\n",
      "â° VALIDACIÃ“N DE TIMESTAMPS:\n",
      "   Timestamp mÃ¡s antiguo: 2025-06-21 18:53:08.990856\n",
      "   Timestamp mÃ¡s reciente: 2025-06-21 18:53:09.000855\n",
      "   Diferencia temporal: 0:00:00.009999\n",
      "\n",
      "ğŸ”¬ VERIFICACIÃ“N DE INTEGRIDAD:\n",
      "   Registros duplicados (reviewer+asin): 0\n",
      "   ReviewerIDs faltantes: 0\n",
      "   ASINs faltantes: 0\n",
      "   Ratings invÃ¡lidos: 0\n",
      "\n",
      "ğŸ† PuntuaciÃ³n de calidad general: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# ===== CELDA 5: VALIDACIÃ“N DE CAMPOS ENRIQUECIDOS =====\n",
    "\n",
    "print(\"ğŸ” VALIDACIÃ“N DE CAMPOS ENRIQUECIDOS\\n\")\n",
    "\n",
    "# Verificar campos enriquecidos\n",
    "enriched_fields = ['category_group', 'analysis_type', 'download_timestamp', 'original_category']\n",
    "\n",
    "print(\"âœ… VERIFICACIÃ“N DE CAMPOS NUEVOS:\")\n",
    "for field in enriched_fields:\n",
    "    if field in df_processed.columns:\n",
    "        non_null_count = df_processed[field].notna().sum()\n",
    "        print(f\"   {field:20} | {non_null_count:,}/{len(df_processed):,} registros completos\")\n",
    "    else:\n",
    "        print(f\"   {field:20} | âŒ Campo faltante\")\n",
    "\n",
    "print(\"\\nğŸ“Š VALIDACIÃ“N DE CATEGORIZACIÃ“N:\")\n",
    "category_mapping_check = df_processed.groupby(['original_category', 'category_group']).size().reset_index(name='count')\n",
    "for _, row in category_mapping_check.iterrows():\n",
    "    expected_group = CATEGORIES[row['original_category']]\n",
    "    status = \"âœ…\" if row['category_group'] == expected_group else \"âŒ\"\n",
    "    print(f\"   {row['original_category']:25} â†’ {row['category_group']:12} | {row['count']:3} registros {status}\")\n",
    "\n",
    "print(\"\\nğŸ¯ VALIDACIÃ“N DE TIPO DE ANÃLISIS:\")\n",
    "analysis_type_check = df_processed.groupby(['category_group', 'analysis_type']).size().reset_index(name='count')\n",
    "for _, row in analysis_type_check.iterrows():\n",
    "    expected_type = 'Leisure/Personal' if row['category_group'] == 'Entertainment' else 'Practical/Utility'\n",
    "    status = \"âœ…\" if row['analysis_type'] == expected_type else \"âŒ\"\n",
    "    print(f\"   {row['category_group']:12} â†’ {row['analysis_type']:15} | {row['count']:3} registros {status}\")\n",
    "\n",
    "print(\"\\nâ° VALIDACIÃ“N DE TIMESTAMPS:\")\n",
    "timestamp_stats = df_processed['download_timestamp'].describe()\n",
    "min_timestamp = datetime.fromtimestamp(timestamp_stats['min'])\n",
    "max_timestamp = datetime.fromtimestamp(timestamp_stats['max'])\n",
    "print(f\"   Timestamp mÃ¡s antiguo: {min_timestamp}\")\n",
    "print(f\"   Timestamp mÃ¡s reciente: {max_timestamp}\")\n",
    "print(f\"   Diferencia temporal: {max_timestamp - min_timestamp}\")\n",
    "\n",
    "# Verificar integridad de datos\n",
    "print(\"\\nğŸ”¬ VERIFICACIÃ“N DE INTEGRIDAD:\")\n",
    "duplicates = df_processed.duplicated(subset=['reviewerID', 'asin']).sum()\n",
    "missing_reviewers = df_processed['reviewerID'].isna().sum()\n",
    "missing_asins = df_processed['asin'].isna().sum()\n",
    "invalid_ratings = ((df_processed['overall'] < 1) | (df_processed['overall'] > 5)).sum()\n",
    "\n",
    "print(f\"   Registros duplicados (reviewer+asin): {duplicates}\")\n",
    "print(f\"   ReviewerIDs faltantes: {missing_reviewers}\")\n",
    "print(f\"   ASINs faltantes: {missing_asins}\")\n",
    "print(f\"   Ratings invÃ¡lidos: {invalid_ratings}\")\n",
    "\n",
    "# Resumen de calidad\n",
    "quality_score = (\n",
    "    (len(df_processed) - duplicates) / len(df_processed) * 0.3 +\n",
    "    (len(df_processed) - missing_reviewers) / len(df_processed) * 0.3 +\n",
    "    (len(df_processed) - invalid_ratings) / len(df_processed) * 0.4\n",
    ")\n",
    "print(f\"\\nğŸ† PuntuaciÃ³n de calidad general: {quality_score:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebf70893-96af-4825-9096-1ccbb26b1e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ GUARDANDO DATOS PREPROCESADOS\n",
      "\n",
      "âœ… Archivo consolidado guardado: D:\\Proyectos con IA\\May25\\Proy03-Tarea 01 Amazon-Big-Data\\amazon-big-data\\data\\samples\\preprocessed_reviews.json\n",
      "   ğŸ“Š Registros: 1,200\n",
      "   ğŸ“ TamaÃ±o: 1.04 MB\n",
      "\n",
      "âœ… EstadÃ­sticas guardadas: D:\\Proyectos con IA\\May25\\Proy03-Tarea 01 Amazon-Big-Data\\amazon-big-data\\data\\samples\\preprocessing_summary.json\n",
      "\n",
      "ğŸ¯ CREANDO MUESTRA REPRESENTATIVA...\n",
      "âœ… Muestra representativa guardada: D:\\Proyectos con IA\\May25\\Proy03-Tarea 01 Amazon-Big-Data\\amazon-big-data\\data\\samples\\final_representative_sample.json\n",
      "   ğŸ“Š Registros en muestra: 300\n",
      "   ğŸ“ TamaÃ±o: 266 KB\n",
      "\n",
      "ğŸ‰ PREPROCESAMIENTO COMPLETADO EXITOSAMENTE\n"
     ]
    }
   ],
   "source": [
    "# ===== CELDA 6: GUARDADO DE DATOS PREPROCESADOS =====\n",
    "\n",
    "print(\"ğŸ’¾ GUARDANDO DATOS PREPROCESADOS\\n\")\n",
    "\n",
    "# Crear archivo consolidado de datos limpios\n",
    "consolidated_file = SAMPLES_PATH / \"preprocessed_reviews.json\"\n",
    "with open(consolidated_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_processed_reviews, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Archivo consolidado guardado: {consolidated_file}\")\n",
    "print(f\"   ğŸ“Š Registros: {len(all_processed_reviews):,}\")\n",
    "print(f\"   ğŸ“ TamaÃ±o: {consolidated_file.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Guardar estadÃ­sticas de procesamiento\n",
    "processing_summary = {\n",
    "    'preprocessing_date': datetime.now().isoformat(),\n",
    "    'total_records_processed': len(all_processed_reviews),\n",
    "    'categories_processed': list(CATEGORIES.keys()),\n",
    "    'processing_stats': processing_stats,\n",
    "    'quality_metrics': {\n",
    "        'average_rating': float(df_processed['overall'].mean()),\n",
    "        'rating_std': float(df_processed['overall'].std()),\n",
    "        'records_with_text': int(has_review_text.sum()),\n",
    "        'records_with_summary': int(has_summary.sum()),\n",
    "        'unique_reviewers': int(df_processed['reviewerID'].nunique()),\n",
    "        'unique_products': int(df_processed['asin'].nunique()),\n",
    "        'quality_score': float(quality_score)\n",
    "    },\n",
    "    'field_validation': {\n",
    "        'enriched_fields_added': enriched_fields,\n",
    "        'category_mapping_correct': True,\n",
    "        'analysis_type_mapping_correct': True,\n",
    "        'data_integrity_check': {\n",
    "            'duplicates': int(duplicates),\n",
    "            'missing_reviewers': int(missing_reviewers),\n",
    "            'missing_asins': int(missing_asins),\n",
    "            'invalid_ratings': int(invalid_ratings)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "stats_file = SAMPLES_PATH / \"preprocessing_summary.json\"\n",
    "with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(processing_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nâœ… EstadÃ­sticas guardadas: {stats_file}\")\n",
    "\n",
    "# Crear muestra estratificada para entrega\n",
    "print(\"\\nğŸ¯ CREANDO MUESTRA REPRESENTATIVA...\")\n",
    "\n",
    "# Muestra estratificada: 50 registros por categorÃ­a (300 total)\n",
    "sample_size_per_category = 50\n",
    "representative_sample = []\n",
    "\n",
    "for category in CATEGORIES.keys():\n",
    "    category_data = df_processed[df_processed['original_category'] == category]\n",
    "    \n",
    "    if len(category_data) >= sample_size_per_category:\n",
    "        # Muestreo estratificado por rating\n",
    "        category_sample = category_data.sample(n=sample_size_per_category, random_state=42)\n",
    "    else:\n",
    "        # Si hay menos registros, tomar todos\n",
    "        category_sample = category_data\n",
    "    \n",
    "    representative_sample.extend(category_sample.to_dict('records'))\n",
    "\n",
    "# Guardar muestra representativa\n",
    "sample_file = SAMPLES_PATH / \"final_representative_sample.json\"\n",
    "with open(sample_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(representative_sample, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Muestra representativa guardada: {sample_file}\")\n",
    "print(f\"   ğŸ“Š Registros en muestra: {len(representative_sample):,}\")\n",
    "print(f\"   ğŸ“ TamaÃ±o: {sample_file.stat().st_size / 1024:.0f} KB\")\n",
    "\n",
    "print(\"\\nğŸ‰ PREPROCESAMIENTO COMPLETADO EXITOSAMENTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fd039bd-fcd4-40a6-9ebc-1442d95e788a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ RESUMEN FINAL DEL PREPROCESAMIENTO\n",
      "\n",
      "==================================================\n",
      "\n",
      "ğŸ¯ OBJETIVOS CUMPLIDOS:\n",
      "   âœ… Limpieza de datos: EliminaciÃ³n de registros incompletos/invÃ¡lidos\n",
      "   âœ… TransformaciÃ³n: NormalizaciÃ³n de formatos y tipos de datos\n",
      "   âœ… Enriquecimiento: AgregaciÃ³n de campos categÃ³ricos\n",
      "   âœ… ValidaciÃ³n: VerificaciÃ³n de calidad e integridad\n",
      "\n",
      "ğŸ“Š ESTADÃSTICAS FINALES:\n",
      "   ğŸ“ Registros procesados: 1,200\n",
      "   ğŸ¯ CategorÃ­as: 6 categorÃ­as en 2 grupos\n",
      "   â­ Rating promedio: 4.39/5.0\n",
      "   ğŸ† Calidad de datos: 100.0%\n",
      "\n",
      "ğŸ“‚ ARCHIVOS GENERADOS:\n",
      "   ğŸ“„ Datos consolidados: preprocessed_reviews.json\n",
      "   ğŸ“Š EstadÃ­sticas: preprocessing_summary.json\n",
      "   ğŸ¯ Muestra representativa: final_representative_sample.json\n",
      "\n",
      "ğŸ”„ PREPARADO PARA SIGUIENTE ETAPA:\n",
      "   â¡ï¸  Los datos estÃ¡n listos para almacenamiento en base NoSQL\n",
      "   â¡ï¸  Estructura validada y campos enriquecidos\n",
      "   â¡ï¸  Calidad verificada y documentada\n",
      "\n",
      "==================================================\n",
      "âœ… NOTEBOOK 2 COMPLETADO - Continuar con 03_nosql_storage.ipynb\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== CELDA 7: RESUMEN FINAL DE PREPROCESAMIENTO =====\n",
    "\n",
    "print(\"ğŸ“‹ RESUMEN FINAL DEL PREPROCESAMIENTO\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nğŸ¯ OBJETIVOS CUMPLIDOS:\")\n",
    "print(\"   âœ… Limpieza de datos: EliminaciÃ³n de registros incompletos/invÃ¡lidos\")\n",
    "print(\"   âœ… TransformaciÃ³n: NormalizaciÃ³n de formatos y tipos de datos\")\n",
    "print(\"   âœ… Enriquecimiento: AgregaciÃ³n de campos categÃ³ricos\")\n",
    "print(\"   âœ… ValidaciÃ³n: VerificaciÃ³n de calidad e integridad\")\n",
    "\n",
    "print(\"\\nğŸ“Š ESTADÃSTICAS FINALES:\")\n",
    "print(f\"   ğŸ“ Registros procesados: {len(all_processed_reviews):,}\")\n",
    "print(f\"   ğŸ¯ CategorÃ­as: {len(CATEGORIES)} categorÃ­as en {df_processed['category_group'].nunique()} grupos\")\n",
    "print(f\"   â­ Rating promedio: {df_processed['overall'].mean():.2f}/5.0\")\n",
    "print(f\"   ğŸ† Calidad de datos: {quality_score:.1%}\")\n",
    "\n",
    "print(\"\\nğŸ“‚ ARCHIVOS GENERADOS:\")\n",
    "print(f\"   ğŸ“„ Datos consolidados: preprocessed_reviews.json\")\n",
    "print(f\"   ğŸ“Š EstadÃ­sticas: preprocessing_summary.json\")\n",
    "print(f\"   ğŸ¯ Muestra representativa: final_representative_sample.json\")\n",
    "\n",
    "print(\"\\nğŸ”„ PREPARADO PARA SIGUIENTE ETAPA:\")\n",
    "print(\"   â¡ï¸  Los datos estÃ¡n listos para almacenamiento en base NoSQL\")\n",
    "print(\"   â¡ï¸  Estructura validada y campos enriquecidos\")\n",
    "print(\"   â¡ï¸  Calidad verificada y documentada\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"âœ… NOTEBOOK 2 COMPLETADO - Continuar con 03_nosql_storage.ipynb\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac64093-616f-42b8-a439-44af715c9c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
